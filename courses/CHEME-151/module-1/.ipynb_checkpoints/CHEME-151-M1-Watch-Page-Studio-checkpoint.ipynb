{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59665570-10e7-4ec4-bf08-422156c35a51",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and Clustering\n",
    "In this module, we will explore unsupervised learning techniques, with a focus on clustering methods. By the end of this module, you will be able to define and demonstrate mastery of the following key concepts:\n",
    "\n",
    "* __Unsupervised learning__: is a type of machine learning that aims to identify patterns and structures in data without explicit guidance. Unsupervised learning is particularly useful when dealing with large volumes of unstructured data or when the desired outcomes are unknown.\n",
    "* __Clustering__ is a typical unsupervised learning technique that divides a dataset into distinct groups or clusters based on the similarity of data points. Clustering algorithms aim to group data points that are more similar to each other than to those in different clusters.\n",
    "* __K-means clustering__ is a popular and straightforward clustering algorithm that partitions a dataset into $k$ clusters. \n",
    "The algorithm iteratively assigns data points to the nearest cluster center and updates the cluster centers based on the mean of the assigned points.\n",
    "\n",
    "Unsupervised learning and clustering are often the starting points for many machine learning projects, particularly when working with large datasets or when the desired outcomes are unknown. By mastering these concepts, you will be well-equipped to tackle a wide range of data analysis tasks. You will have some insights into the structure and relationships within your data (which is handy when building more complex models later on). \n",
    "\n",
    "Let's get started! I'm pumped!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46af15c",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "Unsupervised learning is a branch of machine learning that deals with unlabeled data. Its goal is to discover hidden patterns and structures without predefined target variables. \n",
    "\n",
    "* _How is this different from supervised methods?_ In contrast to supervised methods, which learn a mapping from inputs $\\mathbf{x}$ to known outputs (labels) $y$, unsupervised algorithms seek to model the joint distribution $p(\\mathbf{x})$ or to find a low‐dimensional representation of the data. This often involves making assumptions about the data’s structure, such as cluster membership or probabilistic mixture components, and optimizing an objective that reflects those assumptions.\n",
    "\n",
    "__Clustering__ is an unsupervised machine learning technique that organizes data points into groups, or clusters, based on their _similarities_ without prior knowledge of the group memberships. This method is widely used for exploratory data analysis, enabling the discovery of patterns and relationships within complex datasets. There are several popular clustering algorithms, each with its strengths and weaknesses:\n",
    "\n",
    "* __Hierarchical clustering__ is an unsupervised machine learning technique that organizes data points into a tree-like structure of nested clusters. This allows for the identification of relationships and patterns within the dataset. This method can be implemented using two main approaches: agglomerative, which merges individual points into larger clusters, and divisive, which splits a single cluster into smaller ones.\n",
    "* __Density-based spatial clustering of applications with noise (DBSCAN)__ is a density-based clustering algorithm that groups closely packed data points while effectively identifying outliers, making it particularly useful for datasets with noise and clusters of arbitrary shapes. By defining clusters as dense regions separated by areas of lower density, DBSCAN can efficiently discover meaningful patterns in complex data distributions\n",
    "* __Gaussian mixture models (GMMs)__ are probabilistic models that represent a dataset as a combination of multiple Gaussian distributions, each characterized by its mean and covariance. This allows for the identification of underlying subpopulations within the data. This approach is useful in clustering and density estimation, providing a flexible framework for modeling complex, multimodal distributions.\n",
    "\n",
    "However, let's start at the beginning and look at one of the most basic and widely used clustering algorithms: K-means clustering.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9293ed",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "The [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering), originally developed by [Lloyd in the 1950s but not published until much later in 1982](https://ieeexplore.ieee.org/document/1056489), is our first example of $\\texttt{unsupervised learning}$. Suppose we have a dataset $\\mathcal{D}=\\left\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\dots,\\mathbf{x}_{n}\\in\\mathbb{R}^{m}\\right\\}$ where each $\\mathbf{x}\\in\\mathbb{R}^{m}$ is an $m$-dimensional feature vector.\n",
    "[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is a popular unsupervised machine learning algorithm for clustering data points (feature vectors) $\\mathbf{x}\\in\\mathcal{D}$ into distinct a set of groups (clusters) $\\mathcal{C} = \\left\\{\\mathcal{c}_{1},\\dots,\\mathcal{c}_{K}\\right\\}$ based on _similarity_.\n",
    "\n",
    "* __Similarity?__ Similarity refers to how _close_ data points are to each other in the feature space, i.e., how close $\\mathbf{x}_{i}$ is to $\\mathbf{x}_{j}$ using a distance of similarity measure $d(\\mathbf{x}_{i},\\mathbf{y}_{j})$. What is close? This is a subjective question, and the answer depends on the context of the data and the distance metric $d(\\mathbf{x}_{i},\\mathbf{y}_{j})$ you choose.\n",
    "* __Close features are assumed to be similar!__ The most commonly used similarity measure in [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) is [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). However, we can use other types of measures. The choice of similarity measure can significantly impact the resulting clusters. However, similar features are assumed to be close, and the K-means algorithm will group them accordingly.\n",
    "\n",
    "### Algorithm\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\in\\mathbb{R}^{m}\\}$, where each data point $\\mathbf{x}_i$ is a $m$-dimensional vector. The goal of the K-means algorithm is to partition the dataset into $K$ clusters, $\\mathcal{C} = \\left\\{c_{1},c_{2},\\dots, c_{K}\\right\\}$, \n",
    "where each cluster $c_{j}$ is represented by a centroid $\\mu_j\\in\\mathbb{R}^{m}$. To enforce that $\\mathcal{C}$ is a valid partition of $\\mathcal{D}$, we require that each data point $\\mathbf{x}_i$ is assigned to exactly one cluster $c_{j}$, i.e., $c_{i}\\cap c_{j} = \\emptyset$ for $i\\neq j$ (no shared members).\n",
    "In addition, the union of all clusters covers the entire dataset, i.e., $\\cup_{j=1}^{K}c_{j} = \\mathcal{D}$.\n",
    "The problem of K-means clustering can be formulated as an optimization problem, where the objective is to minimize the sum of squared distances between each data point and its assigned cluster centroid:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{\\mathcal{C}} & \\sum_{j=1}^{K}\\sum_{\\mathbf{x}\\in c_{j}}\\lVert{\\mathbf{x} - \\mu_{j}}\\rVert_{2}^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\lVert{\\star}\\rVert_{2}^{2}$ denotes the squared Euclidean distance (L2 squared) between two vectors (in this case, a feature vector and the cluster centroid). \n",
    "The K-means algorithm seeks to determine the optimal cluster assignments $\\mathcal{C}$ and cluster centroids $\\mu$ that minimize the objective function.\n",
    "\n",
    "Let's look at a simple K-means clustering algorithm (Lloyd's algorithm).\n",
    "\n",
    "__Initialization__. Given a dataset $\\mathcal{D} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\in\\mathbb{R}^{m}\\}$, you specify the number of clusters $K$ and randomly initialize the cluster centroids $\\mu_1, \\mu_2, \\ldots, \\mu_K$. Specify a tolerance $\\epsilon>0$, a maximum number of iterations $T$, an initial iteration $t\\gets{0}$, and the flag $\\texttt{converged}\\gets{\\texttt{false}}$ to indicate whether the algorithm has converged.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "\n",
    "1. __Assignment step__. For each data point $\\mathbf{x}_i\\in\\mathcal{D}$, assign it to the cluster $c_j$ whose centroid $\\mu_j$ is closest to $\\mathbf{x}_i$: compute\n",
    "   $c_{j} \\gets \\arg\\min_{1\\leq{j}\\leq{K}}\\lVert{\\mathbf{x}_{i} - \\mu_{j}}\\rVert_{2}^{2}$. This step creates a partition of the dataset into $K$ clusters based on the nearest centroid.\n",
    "2. __Store__: Store the current cluster centroids: $\\mu_{j}^{(t)} \\gets \\mu_{j}$ for each cluster $c_j$.\n",
    "3. __Update step__: Recompute the centroids based on the current cluster assignments. For each cluster $c_j$, update the centroid $\\mu_j$ by computing the mean of all data points assigned to that cluster: $\\mu_{j} \\gets {|c_{j}|}^{-1}\\sum_{\\mathbf{x}\\in c_{j}}\\mathbf{x}$, where $|c_{j}|$ is the cardinality (number of data points) of cluster $c_j$.\n",
    "4. __Convergence__: Check if the algorithm has converged by evaluating the change in centroids.\n",
    "    - If $\\lVert{\\mu_{j}^{(t)} - \\mu_{j}}\\rVert_{2}^{2} < \\epsilon$ for all clusters $c_j$, then the algorithm has converged, and you can set $\\texttt{converged}\\gets{\\texttt{true}}$. Return the final cluster assignments and centroids.\n",
    "    - If not, increment the iteration counter $t\\gets{t+1}$ and repeat from step 1 until convergence or until the maximum number of iterations $T$ is reached.\n",
    "    - If $t \\geq T$, then set $\\texttt{converged}\\gets{\\texttt{true}}$ to terminate the algorithm, return the last assignments (optionally flagging that convergence tolerance was not met).\n",
    "\n",
    "__Interesting!__: But how do we choose the number of clusters $K$?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac996ffb",
   "metadata": {},
   "source": [
    "## How many clusters should we choose?\n",
    "One of K-means' shortcomings is the need to specify the number of clusters $K$ in advance, which can be addressed with several heuristic methods. \n",
    "There are several methods to estimate the number of clusters, including the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), the [silhouette method](https://en.wikipedia.org/wiki/Silhouette_(clustering)), or performance metrics \n",
    "such as the [Davies-Bouldin index](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index), the [Dunn index](https://en.wikipedia.org/wiki/Dunn_index)\n",
    "or the [Calinski-Harabasz index](https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index).\n",
    "\n",
    "Let's look the silhouette method, which is a measure of how similar an object is to its own cluster compared to other clusters, and the Calinski-Harabasz index, which is a measure of the ratio of the sum of between-cluster dispersion to within-cluster dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096317e",
   "metadata": {},
   "source": [
    "### Silhouette Method\n",
    "The silhouette method is a technique used to evaluate the quality of clustering by measuring how similar an object is to its own cluster compared to other clusters. The silhouette score balances cohesion and separation in a single metric, facilitating automated selection of $K$. For a data point $i$, the silhouette score $s(i)$ is defined as:\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "where:\n",
    "- __Cohesion__: $a(i)$ is the average distance between the data point $i$ and all other points in the same cluster (cohesion, intra-cluster distance):\n",
    "    $$\n",
    "    a(i) = \\frac{1}{|c_i| - 1} \\sum_{j \\in c_i, j \\neq i} d(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "    $$\n",
    "    A **small** value of $a(i)$ means point $i$ lies close to its fellow cluster members (high cohesion).  A **large** value of $a(i)$ means point $i$ is spread out within its cluster (low cohesion). Intuitively, high cohesion implies that members of the same cluster are very similar (or near one another in feature space), which is one half of what the silhouette score balances against **separation** (the distance to the next‐closest cluster).\n",
    "\n",
    "- __Separation__: $b(i)$ is the average distance between the data point $i$ and all points in the nearest cluster (separation, inter-cluster distance):\n",
    "    $$\n",
    "    b(i) = \\min_{j \\neq c_i} \\frac{1}{|c_j|} \\sum_{k \\in c_j} d(\\mathbf{x}_i, \\mathbf{x}_k)\n",
    "    $$\n",
    "    A **small** $b(i)$ means that there is another cluster whose members lie very close to $i$: point $i$ is not well separated and may be near or across the boundary of another cluster (poor inter-cluster separation). A **large** $b(i)$ means that the nearest other cluster is far away: point $i$ sits well apart from any competing cluster (strong inter-cluster separation), contributing positively to its silhouette score.\n",
    "\n",
    "The silhouette score ranges from `-1` to `1`:\n",
    "- A score close to `1` indicates that the data point is well-clustered and is far from the nearest cluster.\n",
    "- A score close to `0` indicates that the data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "- A negative score indicates that the data point may have been assigned to the wrong cluster.\n",
    "\n",
    "Now, let's look at the Calinski-Harabasz index, which is a measure of the ratio of the sum of between-cluster dispersion to within-cluster dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0888c",
   "metadata": {},
   "source": [
    "### The Calinski-Harabasz Index\n",
    "The Calinski-Harabasz index (also known as the Variance Ratio Criterion) is a metric used to evaluate the quality of clustering by measuring the ratio of between-cluster dispersion to within-cluster dispersion. Let $N=\\sum_{k=1}^K|c_k|$ and $\\mu=\\tfrac1N\\sum_{i=1}^N x_i$. The unnormalized Calinski–Harabasz index is given by:\n",
    "$$\n",
    "\\mathrm{CH}\n",
    "=\\frac{B}{W}\n",
    "= \\frac{\\sum_{k=1}^K |c_k|\\|\\mu_k - \\mu\\|_2^2}\n",
    "        {\\sum_{k=1}^K \\sum_{x_i\\in c_k}\\|x_i - \\mu_k\\|_2^2}.\n",
    "$$\n",
    "where:\n",
    "- $B$ is the between-cluster dispersion, which measures how far apart the cluster centroids are from the overall mean $\\mu$ of the dataset.\n",
    "- $W$ is the within-cluster dispersion, which measures how spread out the data points are within each cluster $c_k$.\n",
    "- $|c_k|$ is the number of data points in cluster $c_k$\n",
    "- $\\mu_k$ is the centroid of cluster $c_k$\n",
    "- $\\mu$ is the overall mean of the dataset.  \n",
    "\n",
    "Optionally, one can normalize by degrees of freedom:\n",
    "$$\n",
    " \\mathrm{CH}\n",
    " = \\frac{B/(K-1)}{W/(N-K)}.\n",
    "$$\n",
    "\n",
    "\n",
    "A higher CH indicates that clusters are both well-separated (characterized by large between-cluster dispersion) and compact (characterized by small within-cluster dispersion). It’s a popular metric for choosing $K$ because it combines these two desirable properties in a single score.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e27f2f-d8a4-4bec-b2ba-36cec2b49ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
