{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836948e7",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition of Systems and Data\n",
    "In this module, we'll continue our discussion of eigendecomposition and explore a powerful technique called [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition), which is closely related to eigendecomposition. There are several key ideas in this lecture:\n",
    "\n",
    "* __Matrix factorization__ decomposes a matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ into the product of two (or more) matrices. This is particularly interesting because such decompositions reveal latent structure, which is useful for applications such as dimensionality reduction, and they underpin key applications, including collaborative filtering in recommender systems, principal component analysis, and topic modeling.\n",
    "*  __Singular Value Decomposition (SVD)__ is a fundamental technique in linear algebra that decomposes a matrix $\\mathbf{A}\\in\\mathbb{C}^{m\\times{n}}$ into three distinct matrices, $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$. The matrices $\\mathbf{U}$ and $\\mathbf{V}$  are orthogonal matrices containing the left and right singular vectors, respectively, while the $\\mathbf{\\Sigma}$ is a diagonal matrix containing the singular values.\n",
    "* __Numerical Stability__. SVD is more stable than eigendecomposition, especially in Principal Component Analysis (PCA), making it preferable for machine learning applications. Many SVD implementations are more stable than the naive QR iteration method we developed for eigendecomposition.\n",
    "\n",
    "Singular Value Decomposition is a powerful tool for analyzing and understanding the structure of matrices, especially in the context of data analysis and machine learning. Let's get started!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ac521-ae53-43ac-83a8-c30650923f88",
   "metadata": {},
   "source": [
    "## Singular value decomposition\n",
    "Let $\\mathbf{A}\\in\\mathbb{R}^{m\\times{n}}$. Its singular value decomposition _factors_ the matrix $\\mathbf{A}$ into three components, each with a different amount of information:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}\n",
    "$$\n",
    "Thus, the matrix is _telling us_ what is important; we just need to listen!\n",
    "* The matrices $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices whose columns are the eigenvectors of $\\mathbf{A}\\mathbf{A}^\\top$ (left singular vectors) and $\\mathbf{A}^\\top\\mathbf{A}$ (right singular vectors), respectively\n",
    "* The matrix $\\mathbf{\\Sigma}\\in\\mathbb{R}^{m\\times{n}}$ is a rectangular diagonal matrix, i.e., zeros off the main diagonal, containing the singular values $\\sigma_{i}=\\Sigma_{ii}$ along the main diagonal. The singular values are the square roots of the eigenvalues of the matrix $\\mathbf{A}^{\\top}\\mathbf{A}$, i.e., $\\sigma_i = \\sqrt{\\lambda_i(\\mathbf{A}^\\top\\mathbf{A})}$.\n",
    "* The number of non-zero singular values is [the rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) of the matrix $\\mathbf{A}$, where $r(\\mathbf{A}) \\leq\\min\\left(n,m\\right)$.\n",
    "\n",
    "#### Why is this interesting?\n",
    "Singular value decomposition can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices, e.g., frames of a larger image, where each component has rank `1`. Let $\\mathbf{A}\\in\\mathbb{R}^{m\\times{n}}$ have the singular value decomposition $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$. Then, the matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{n}}$ can be written as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} & = \\sum_{i=1}^{r({\\mathbf{A}})}\\sigma_{i}\\cdot\\underbrace{\\left(\\mathbf{u}_{i}\\otimes\\mathbf{v}_{i}\\right)}_{\\mathbf{u}_{i}\\mathbf{v}^{\\top}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $r({\\mathbf{A}})$ is the rank of matrix $\\mathbf{A}$, the vectors $\\mathbf{u}_{i}$ and $\\mathbf{v}_{i}$ are the ith left and right singular vectors, and $\\sigma_{i}$ are the ordered singular values. The [outer-product](https://en.wikipedia.org/wiki/Outer_product) $\\left(\\mathbf{u}_{i}\\otimes\\mathbf{v}_{i}\\right)= \\mathbf{u}_{i}\\mathbf{v}^{\\top}_{i}$ is the separable component of the matrix $\\mathbf{A}$ with rank `1`.\n",
    "\n",
    "* __Low‚Äêrank approximation:__ by truncating the sum at $k<r({\\mathbf{A}})$, SVD yields the best rank-$k$ approximation in both the Frobenius and spectral norms (basis for PCA, image compression, etc.).\n",
    "* __Numerical robustness:__ SVD provides a stable way to compute pseudoinverses, solve ill-conditioned systems, and detect the effective rank of $\\mathbf{A}$.\n",
    "\n",
    "Thus, singular value decomposition is helpful in solving linear systems and is the idea underlying data reduction techniques such as principal component analysis (PCA). There are many reasons to be interested!\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
